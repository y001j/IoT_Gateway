package rules

import (
	"context"
	"encoding/json"
	"fmt"
	"sync"
	"sync/atomic"
	"time"

	"github.com/rs/zerolog"
	"github.com/rs/zerolog/log"
)

// MonitoringRuleError æ‰©å±•è§„åˆ™é”™è¯¯ï¼ˆç”¨äºç›‘æ§ï¼‰
type MonitoringRuleError struct {
	*RuleError
	ID          string            `json:"id"`
	RuleID      string            `json:"rule_id,omitempty"`
	ActionType  string            `json:"action_type,omitempty"`
	StackTrace  string            `json:"stack_trace,omitempty"`
	Retry       bool              `json:"retry"`
	RetryCount  int               `json:"retry_count,omitempty"`
	LastRetryAt time.Time         `json:"last_retry_at,omitempty"`
}

// RuleMonitor è§„åˆ™å¼•æ“ç›‘æ§å™¨
type RuleMonitor struct {
	mu                sync.RWMutex
	metrics           *EngineMetrics
	errors            []*MonitoringRuleError
	maxErrors         int
	errorsByType      map[ErrorType]int64
	errorsByLevel     map[ErrorLevel]int64
	actionStats       map[string]*ActionStats
	ruleStats         map[string]*RuleStats
	performanceStats  *PerformanceStats
	healthStatus      HealthStatus
	healthChecks      map[string]HealthChecker
	alertThresholds   *AlertThresholds
	notificationChan  chan *MonitoringRuleError
	ctx               context.Context
	cancel            context.CancelFunc
	logger            zerolog.Logger
}

// ActionStats åŠ¨ä½œç»Ÿè®¡
type ActionStats struct {
	TotalExecutions int64         `json:"total_executions"`
	SuccessCount    int64         `json:"success_count"`
	ErrorCount      int64         `json:"error_count"`
	TotalDuration   time.Duration `json:"total_duration"`
	AvgDuration     time.Duration `json:"avg_duration"`
	MaxDuration     time.Duration `json:"max_duration"`
	MinDuration     time.Duration `json:"min_duration"`
	LastExecuted    time.Time     `json:"last_executed"`
	LastError       string        `json:"last_error,omitempty"`
}

// RuleStats è§„åˆ™ç»Ÿè®¡
type RuleStats struct {
	TotalEvaluations int64         `json:"total_evaluations"`
	MatchCount       int64         `json:"match_count"`
	ErrorCount       int64         `json:"error_count"`
	TotalDuration    time.Duration `json:"total_duration"`
	AvgDuration      time.Duration `json:"avg_duration"`
	LastEvaluated    time.Time     `json:"last_evaluated"`
	LastMatched      time.Time     `json:"last_matched"`
	LastError        string        `json:"last_error,omitempty"`
}

// PerformanceStats æ€§èƒ½ç»Ÿè®¡
type PerformanceStats struct {
	ThroughputPerSecond float64       `json:"throughput_per_second"`
	P50Duration         time.Duration `json:"p50_duration"`
	P95Duration         time.Duration `json:"p95_duration"`
	P99Duration         time.Duration `json:"p99_duration"`
	MemoryUsage         int64         `json:"memory_usage"`
	GoroutineCount      int           `json:"goroutine_count"`
	QueueLength         int           `json:"queue_length"`
	LastUpdated         time.Time     `json:"last_updated"`
}

// HealthStatus å¥åº·çŠ¶æ€
type HealthStatus struct {
	Status       string            `json:"status"` // healthy, degraded, unhealthy
	Message      string            `json:"message"`
	LastChecked  time.Time         `json:"last_checked"`
	CheckResults map[string]string `json:"check_results"`
}

// HealthChecker å¥åº·æ£€æŸ¥å™¨æ¥å£
type HealthChecker interface {
	Name() string
	Check(ctx context.Context) error
}

// AlertThresholds æŠ¥è­¦é˜ˆå€¼
type AlertThresholds struct {
	ErrorRateThreshold    float64       `json:"error_rate_threshold"`
	LatencyThreshold      time.Duration `json:"latency_threshold"`
	ThroughputThreshold   float64       `json:"throughput_threshold"`
	MemoryThreshold       int64         `json:"memory_threshold"`
	QueueLengthThreshold  int           `json:"queue_length_threshold"`
	ConsecutiveErrors     int           `json:"consecutive_errors"`
	CheckInterval         time.Duration `json:"check_interval"`
}

// NewRuleMonitor åˆ›å»ºè§„åˆ™ç›‘æ§å™¨
func NewRuleMonitor(maxErrors int) *RuleMonitor {
	ctx, cancel := context.WithCancel(context.Background())
	
	monitor := &RuleMonitor{
		metrics:          &EngineMetrics{},
		errors:           make([]*MonitoringRuleError, 0, maxErrors),
		maxErrors:        maxErrors,
		errorsByType:     make(map[ErrorType]int64),
		errorsByLevel:    make(map[ErrorLevel]int64),
		actionStats:      make(map[string]*ActionStats),
		ruleStats:        make(map[string]*RuleStats),
		performanceStats: &PerformanceStats{},
		healthStatus: HealthStatus{
			Status:       "healthy",
			Message:      "ç³»ç»Ÿæ­£å¸¸è¿è¡Œ",
			CheckResults: make(map[string]string),
		},
		healthChecks:    make(map[string]HealthChecker),
		alertThresholds: getDefaultAlertThresholds(),
		notificationChan: make(chan *MonitoringRuleError, 100),
		ctx:             ctx,
		cancel:          cancel,
		logger:          log.With().Str("component", "rule_monitor").Logger(),
	}
	
	// å¯åŠ¨ç›‘æ§åç¨‹
	go monitor.run()
	
	return monitor
}

// RecordError è®°å½•é”™è¯¯
func (m *RuleMonitor) RecordError(errType ErrorType, level ErrorLevel, message, details string, context map[string]string) {
	baseError := NewRuleError(errType, level, "", message).
		WithDetails(details).
		SetRetryable(shouldRetry(errType, level))
	
	// æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
	for k, v := range context {
		baseError.WithContext(k, v)
	}
	
	monitoringError := &MonitoringRuleError{
		RuleError: baseError,
		ID:        generateErrorID(),
		Retry:     shouldRetry(errType, level),
	}
	
	m.mu.Lock()
	defer m.mu.Unlock()
	
	// æ·»åŠ åˆ°é”™è¯¯åˆ—è¡¨ï¼ˆä¿æŒæœ€å¤§æ•°é‡é™åˆ¶ï¼‰
	if len(m.errors) >= m.maxErrors {
		// ç§»é™¤æœ€æ—§çš„é”™è¯¯
		m.errors = m.errors[1:]
	}
	m.errors = append(m.errors, monitoringError)
	
	// æ›´æ–°ç»Ÿè®¡
	m.errorsByType[errType]++
	m.errorsByLevel[level]++
	
	// è®°å½•æ—¥å¿—
	m.logError(monitoringError)
	
	// å‘é€é€šçŸ¥ï¼ˆéé˜»å¡ï¼‰
	select {
	case m.notificationChan <- monitoringError:
	default:
		m.logger.Warn().Msg("é”™è¯¯é€šçŸ¥é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒé”™è¯¯é€šçŸ¥")
	}
}

// RecordRuleExecution è®°å½•è§„åˆ™æ‰§è¡Œ
func (m *RuleMonitor) RecordRuleExecution(ruleID string, duration time.Duration, matched bool, err error) {
	log.Debug().
		Str("rule_id", ruleID).
		Bool("matched", matched).
		Err(err).
		Msg("ğŸ“Š RuleMonitor.RecordRuleExecutionè¢«è°ƒç”¨")
	
	m.mu.Lock()
	defer m.mu.Unlock()
	
	// å¢åŠ è°ƒè¯•ï¼šè®°å½•åŸå­æ“ä½œå‰åçš„å€¼
	oldValue := atomic.LoadInt64(&m.metrics.PointsProcessed)
	atomic.AddInt64(&m.metrics.PointsProcessed, 1)
	newValue := atomic.LoadInt64(&m.metrics.PointsProcessed)
	
	log.Debug().
		Int64("points_processed_before", oldValue).
		Int64("points_processed_after", newValue).
		Msg("ğŸ”¢ PointsProcessedåŸå­è®¡æ•°å™¨æ›´æ–°")
	
	if stats, exists := m.ruleStats[ruleID]; exists {
		stats.TotalEvaluations++
		stats.TotalDuration += duration
		stats.AvgDuration = stats.TotalDuration / time.Duration(stats.TotalEvaluations)
		stats.LastEvaluated = time.Now()
		
		if matched {
			stats.MatchCount++
			stats.LastMatched = time.Now()
			atomic.AddInt64(&m.metrics.RulesMatched, 1)
		}
		
		if err != nil {
			stats.ErrorCount++
			stats.LastError = err.Error()
		}
	} else {
		stats := &RuleStats{
			TotalEvaluations: 1,
			TotalDuration:    duration,
			AvgDuration:      duration,
			LastEvaluated:    time.Now(),
		}
		
		if matched {
			stats.MatchCount = 1
			stats.LastMatched = time.Now()
			atomic.AddInt64(&m.metrics.RulesMatched, 1)
		}
		
		if err != nil {
			stats.ErrorCount = 1
			stats.LastError = err.Error()
		}
		
		m.ruleStats[ruleID] = stats
	}
	
	m.metrics.LastProcessedAt = time.Now()
}

// RecordActionExecution è®°å½•åŠ¨ä½œæ‰§è¡Œ
func (m *RuleMonitor) RecordActionExecution(actionType string, duration time.Duration, success bool, err error) {
	m.mu.Lock()
	defer m.mu.Unlock()
	
	atomic.AddInt64(&m.metrics.ActionsExecuted, 1)
	
	if stats, exists := m.actionStats[actionType]; exists {
		stats.TotalExecutions++
		stats.TotalDuration += duration
		stats.AvgDuration = stats.TotalDuration / time.Duration(stats.TotalExecutions)
		stats.LastExecuted = time.Now()
		
		if duration > stats.MaxDuration {
			stats.MaxDuration = duration
		}
		if stats.MinDuration == 0 || duration < stats.MinDuration {
			stats.MinDuration = duration
		}
		
		if success {
			stats.SuccessCount++
			atomic.AddInt64(&m.metrics.ActionsSucceeded, 1)
		} else {
			stats.ErrorCount++
			atomic.AddInt64(&m.metrics.ActionsFailed, 1)
			if err != nil {
				stats.LastError = err.Error()
			}
		}
	} else {
		stats := &ActionStats{
			TotalExecutions: 1,
			TotalDuration:   duration,
			AvgDuration:     duration,
			MaxDuration:     duration,
			MinDuration:     duration,
			LastExecuted:    time.Now(),
		}
		
		if success {
			stats.SuccessCount = 1
			atomic.AddInt64(&m.metrics.ActionsSucceeded, 1)
		} else {
			stats.ErrorCount = 1
			atomic.AddInt64(&m.metrics.ActionsFailed, 1)
			if err != nil {
				stats.LastError = err.Error()
			}
		}
		
		m.actionStats[actionType] = stats
	}
}

// GetMetrics è·å–ç›‘æ§æŒ‡æ ‡
func (m *RuleMonitor) GetMetrics() *EngineMetrics {
	m.mu.RLock()
	defer m.mu.RUnlock()
	
	// åˆ›å»ºå‰¯æœ¬ä»¥é¿å…å¹¶å‘è®¿é—®é—®é¢˜
	metrics := &EngineMetrics{
		RulesTotal:         m.metrics.RulesTotal,
		RulesEnabled:       m.metrics.RulesEnabled,
		PointsProcessed:    atomic.LoadInt64(&m.metrics.PointsProcessed),
		RulesMatched:       atomic.LoadInt64(&m.metrics.RulesMatched),
		ActionsExecuted:    atomic.LoadInt64(&m.metrics.ActionsExecuted),
		ActionsSucceeded:   atomic.LoadInt64(&m.metrics.ActionsSucceeded),
		ActionsFailed:      atomic.LoadInt64(&m.metrics.ActionsFailed),
		ProcessingDuration: m.metrics.ProcessingDuration,
		LastProcessedAt:    m.metrics.LastProcessedAt,
	}
	
	return metrics
}

// GetErrors è·å–é”™è¯¯åˆ—è¡¨
func (m *RuleMonitor) GetErrors(limit int) []*MonitoringRuleError {
	m.mu.RLock()
	defer m.mu.RUnlock()
	
	if limit <= 0 || limit > len(m.errors) {
		limit = len(m.errors)
	}
	
	// è¿”å›æœ€æ–°çš„é”™è¯¯ï¼ˆå€’åºï¼‰
	result := make([]*MonitoringRuleError, limit)
	start := len(m.errors) - limit
	for i := 0; i < limit; i++ {
		result[i] = m.errors[start+i]
	}
	
	return result
}

// GetHealthStatus è·å–å¥åº·çŠ¶æ€
func (m *RuleMonitor) GetHealthStatus() HealthStatus {
	m.mu.RLock()
	defer m.mu.RUnlock()
	return m.healthStatus
}

// RegisterHealthChecker æ³¨å†Œå¥åº·æ£€æŸ¥å™¨
func (m *RuleMonitor) RegisterHealthChecker(checker HealthChecker) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.healthChecks[checker.Name()] = checker
}

// run è¿è¡Œç›‘æ§å™¨
func (m *RuleMonitor) run() {
	ticker := time.NewTicker(m.alertThresholds.CheckInterval)
	defer ticker.Stop()
	
	for {
		select {
		case <-m.ctx.Done():
			return
		case <-ticker.C:
			m.performHealthChecks()
			m.updatePerformanceStats()
			m.checkAlerts()
		case err := <-m.notificationChan:
			m.handleErrorNotification(err)
		}
	}
}

// performHealthChecks æ‰§è¡Œå¥åº·æ£€æŸ¥
func (m *RuleMonitor) performHealthChecks() {
	m.mu.Lock()
	defer m.mu.Unlock()
	
	checkResults := make(map[string]string)
	overallStatus := "healthy"
	var messages []string
	
	for name, checker := range m.healthChecks {
		ctx, cancel := context.WithTimeout(m.ctx, 5*time.Second)
		err := checker.Check(ctx)
		cancel()
		
		if err != nil {
			checkResults[name] = fmt.Sprintf("FAIL: %s", err.Error())
			overallStatus = "unhealthy"
			messages = append(messages, fmt.Sprintf("%sæ£€æŸ¥å¤±è´¥: %s", name, err.Error()))
		} else {
			checkResults[name] = "PASS"
		}
	}
	
	m.healthStatus.CheckResults = checkResults
	m.healthStatus.LastChecked = time.Now()
	
	if overallStatus == "unhealthy" {
		m.healthStatus.Status = "unhealthy"
		m.healthStatus.Message = fmt.Sprintf("å¥åº·æ£€æŸ¥å¤±è´¥: %v", messages)
	} else {
		m.healthStatus.Status = "healthy"
		m.healthStatus.Message = "æ‰€æœ‰å¥åº·æ£€æŸ¥é€šè¿‡"
	}
}

// updatePerformanceStats æ›´æ–°æ€§èƒ½ç»Ÿè®¡
func (m *RuleMonitor) updatePerformanceStats() {
	// è¿™é‡Œå¯ä»¥æ·»åŠ æ€§èƒ½ç»Ÿè®¡çš„æ›´æ–°é€»è¾‘
	// ä¾‹å¦‚ï¼šè®¡ç®—ååé‡ã€å»¶è¿Ÿç™¾åˆ†ä½æ•°ç­‰
	m.mu.Lock()
	defer m.mu.Unlock()
	
	m.performanceStats.LastUpdated = time.Now()
	// TODO: å®ç°è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡è®¡ç®—
}

// checkAlerts æ£€æŸ¥æŠ¥è­¦æ¡ä»¶
func (m *RuleMonitor) checkAlerts() {
	m.mu.RLock()
	defer m.mu.RUnlock()
	
	// æ£€æŸ¥é”™è¯¯ç‡
	totalActions := atomic.LoadInt64(&m.metrics.ActionsExecuted)
	failedActions := atomic.LoadInt64(&m.metrics.ActionsFailed)
	
	if totalActions > 0 {
		errorRate := float64(failedActions) / float64(totalActions)
		if errorRate > m.alertThresholds.ErrorRateThreshold {
			m.logger.Warn().
				Float64("error_rate", errorRate).
				Float64("threshold", m.alertThresholds.ErrorRateThreshold).
				Msg("åŠ¨ä½œé”™è¯¯ç‡è¶…è¿‡é˜ˆå€¼")
		}
	}
	
	// æ£€æŸ¥é˜Ÿåˆ—é•¿åº¦
	if m.performanceStats.QueueLength > m.alertThresholds.QueueLengthThreshold {
		m.logger.Warn().
			Int("queue_length", m.performanceStats.QueueLength).
			Int("threshold", m.alertThresholds.QueueLengthThreshold).
			Msg("é˜Ÿåˆ—é•¿åº¦è¶…è¿‡é˜ˆå€¼")
	}
}

// handleErrorNotification å¤„ç†é”™è¯¯é€šçŸ¥
func (m *RuleMonitor) handleErrorNotification(ruleError *MonitoringRuleError) {
	// è¿™é‡Œå¯ä»¥æ·»åŠ é”™è¯¯é€šçŸ¥çš„å¤„ç†é€»è¾‘
	// ä¾‹å¦‚ï¼šå‘é€åˆ°å¤–éƒ¨ç›‘æ§ç³»ç»Ÿã€è§¦å‘æŠ¥è­¦ç­‰
	
	if ruleError.Level == ErrorLevelCritical || ruleError.Level == ErrorLevelError {
		m.logger.Error().
			Str("error_id", ruleError.ID).
			Str("type", string(ruleError.Type)).
			Str("message", ruleError.Message).
			Msg("ä¸¥é‡é”™è¯¯é€šçŸ¥")
	}
}

// logError è®°å½•é”™è¯¯æ—¥å¿—
func (m *RuleMonitor) logError(ruleError *MonitoringRuleError) {
	logEvent := m.logger.With().
		Str("error_id", ruleError.ID).
		Str("type", string(ruleError.Type)).
		Str("level", string(ruleError.Level)).
		Str("message", ruleError.Message).
		Time("timestamp", ruleError.Timestamp)
	
	if ruleError.RuleID != "" {
		logEvent = logEvent.Str("rule_id", ruleError.RuleID)
	}
	
	if ruleError.ActionType != "" {
		logEvent = logEvent.Str("action_type", ruleError.ActionType)
	}
	
	if ruleError.Details != "" {
		logEvent = logEvent.Str("details", ruleError.Details)
	}
	
	logger := logEvent.Logger()
	
	switch ruleError.Level {
	case ErrorLevelInfo:
		logger.Info().Msg("è§„åˆ™å¼•æ“ä¿¡æ¯")
	case ErrorLevelWarning:
		logger.Warn().Msg("è§„åˆ™å¼•æ“è­¦å‘Š")
	case ErrorLevelError:
		logger.Error().Msg("è§„åˆ™å¼•æ“é”™è¯¯")
	case ErrorLevelCritical:
		logger.Error().Msg("è§„åˆ™å¼•æ“ä¸¥é‡é”™è¯¯")
	}
}

// Close å…³é—­ç›‘æ§å™¨
func (m *RuleMonitor) Close() {
	m.cancel()
	close(m.notificationChan)
}

// è¾…åŠ©å‡½æ•°

func generateErrorID() string {
	return fmt.Sprintf("err_%d", time.Now().UnixNano())
}

func shouldRetry(errType ErrorType, level ErrorLevel) bool {
	// ç¡®å®šå“ªäº›é”™è¯¯ç±»å‹å’Œçº§åˆ«åº”è¯¥é‡è¯•
	if level == ErrorLevelCritical || level == ErrorLevelError {
		return errType == ErrorTypeTimeout || errType == ErrorTypeSystem
	}
	return false
}

func getDefaultAlertThresholds() *AlertThresholds {
	return &AlertThresholds{
		ErrorRateThreshold:    0.05, // 5%
		LatencyThreshold:      1 * time.Second,
		ThroughputThreshold:   100, // æ¯ç§’å¤„ç†æ•°é‡
		MemoryThreshold:       1024 * 1024 * 1024, // 1GB
		QueueLengthThreshold:  1000,
		ConsecutiveErrors:     5,
		CheckInterval:         30 * time.Second,
	}
}

// ToJSON å°†ç›‘æ§æ•°æ®è½¬æ¢ä¸ºJSON
func (m *RuleMonitor) ToJSON() ([]byte, error) {
	m.mu.RLock()
	defer m.mu.RUnlock()
	
	data := map[string]interface{}{
		"metrics":           m.GetMetrics(),
		"health_status":     m.healthStatus,
		"performance_stats": m.performanceStats,
		"error_stats": map[string]interface{}{
			"by_type":  m.errorsByType,
			"by_level": m.errorsByLevel,
		},
		"action_stats": m.actionStats,
		"rule_stats":   m.ruleStats,
		"recent_errors": m.GetErrors(10),
	}
	
	return json.Marshal(data)
}